Directory structure:
└── database/
    ├── backup-restore.mdx
    ├── document-ids.mdx
    ├── pagination.mdx
    ├── schemas.mdx
    ├── types.md
    ├── writing-data.mdx
    ├── advanced/
    │   ├── _category_.json
    │   ├── occ.md
    │   ├── schema-philosophy.md
    │   └── system-tables.mdx
    ├── import-export/
    │   ├── export.mdx
    │   ├── import-export.mdx
    │   └── import.mdx
    └── reading-data/
        ├── filters.mdx
        ├── reading-data.mdx
        └── indexes/
            ├── indexes-and-query-perf.md
            └── indexes.md

================================================
FILE: npm-packages/docs/docs/database/backup-restore.mdx
================================================
---
title: "Backup & Restore"
sidebar_position: 85
description: "Backup and restore your Convex data and files"
---

Convex supports Backup & Restore of data via the
[dashboard](https://dashboard.convex.dev/deployment/settings/backups).

![Backups Page](/screenshots/backups.png)

# Backups

A backup is a consistent snapshot of your table data and file storage made at
the time of your request.

Take a manual backup by pressing the "Backup Now" button. This may take a few
seconds to a few hours, depending on how much data is in your deployment.

Manual backups are stored for 7 days. You can download or delete backups via
this page.

Deployment configuration and other data (code, environment variables, scheduled
functions, etc.) will not be included.

### Periodic Backups

Schedule a periodic daily or weekly backup by checking the "Backup
automatically" box. You can select what time of day / day of week to have the
backup occur.

Daily backups are stored for 7 days. Weekly backups are stored for 14 days.

<ProFeatureUpsell feature="Periodic backups" verb="require" />

### Restoring from backup

Restore from a backup by selecting "Restore" from the submenu of an individual
backup. You can restore from backups in the same deployment or from other
deployments on the same team by using the deployment selector on the backups
page. Restores may take a few seconds to a few hours depending on how much data
is in your backup.

Note that restoring is a destructive operation that wipes your existing data and
replaces it with that from the backup. It's recommended that you generate an
additional backup before doing a restore.

### Restoring in an emergency

If your production deployment ends up in a bad state, you may want to consider
doing a restore to return to a good state. Note that getting your data to a good
state may not be enough. Consider whether you may need each of the following
actions. Depending on the nature of your emergency, these may be required.

- Take an additional backup prior to restore, since restores are destructive
- Do a restore from a good backup - to restore data
- Use `npx convex dev` to push a known version of good code.
- Use `npx convex env` or the dashboard to restore to a good set of env vars
- Use the dashboard to make any manual fixes to the database for your app.
- Write mutations to make required (more programmatic) manual fixes to the
  database for your app.

# Downloading a backup

You can download your manual and periodic backups from the dashboard via the
download button in the menu.

Alternatively, you can generate an export in the same format with the
[command line](/cli.md#export-data-to-a-file):

```sh
npx convex export --path ~/Downloads
```

The backup comes as a generated a ZIP file with all documents in all Convex
tables in your deployment.

The ZIP file's name has the format `snapshot_{ts}.zip` where `ts` is a UNIX
timestamp of the snapshot in nanoseconds. The export ZIP file contains documents
for each table at `<table_name>/documents.jsonl`, with one document per line.

Exported ZIP files also contain data from [file storage](/file-storage) in a
`_storage` folder, with metadata like IDs and checksums in
`_storage/documents.jsonl` and each file as `_storage/<id>`.

### Using the downloaded backup.

Downloaded ZIP files can be imported into the same deployment or a different
deployment
[with the CLI](/database/import-export/import.mdx#restore-data-from-a-backup-zip-file).

## FAQ

### Are there any limitations?

Each backup is accessible for up to 7 days.

On the Free/Starter plan, up to two backups can stored per deployment at a time.
Deployments on Convex Professional plan can have many backups with standard
usage based pricing.

### How are they priced?

Backups uses database bandwidth to read all documents, and file bandwidth to
include user files. The generation and storage of the backup itself is billed
with the same bandwidth and storage pricing as user file storage. You can
observe this bandwidth and storage cost in the
[usage dashboard](https://dashboard.convex.dev/team/settings/usage). Check the
[limits docs](/production/state/limits#database) for pricing details.

### What does the backup not contain?

The backup only contains the documents for your tables and files in file
storage. In particular it lacks:

1. Your deployment's code and configuration. Convex functions, crons.ts,
   auth.config.js, schema.ts, etc. are configured in your source code.
2. Pending scheduled functions. You can access pending scheduled functions in
   the [`_scheduled_functions`](/database/advanced/system-tables.mdx) system
   table.
3. Environment variables. Environment variables can be copied from Settings in
   the Convex dashboard.



================================================
FILE: npm-packages/docs/docs/database/document-ids.mdx
================================================
---
title: "Document IDs"
sidebar_position: 10
description: "Create complex, relational data models using IDs"
---

import SerializeExample from "!!raw-loader!@site/../private-demos/snippets/convex/tasks.ts";
import SerializeCall from "!!raw-loader!@site/../private-demos/snippets/src/documentIdsSerializeCall.tsx";

**Example:**
[Relational Data Modeling](https://github.com/get-convex/convex-demos/tree/main/relational-data-modeling)

Every document in convex has a globally unique string _document ID_ that is
automatically generated by the system.

```ts
const userId = await ctx.db.insert("users", { name: "Michael Jordan" });
```

You can use this ID to efficiently read a single document using the `get`
method:

```ts
const retrievedUser = await ctx.db.get(userId);
```

You can access the ID of a document in the
[`_id` field](/database/types.md#system-fields):

```ts
const userId = retrievedUser._id;
```

Also, this same ID can be used to update that document in place:

```ts
await ctx.db.patch(userId, { name: "Steph Curry" });
```

Convex generates an [`Id`](/generated-api/data-model#id) TypeScript type based
on your [schema](/database/schemas.mdx) that is parameterized over your table
names:

```typescript
import { Id } from "./_generated/dataModel";

const userId: Id<"users"> = user._id;
```

IDs are strings at runtime, but the [`Id`](/generated-api/data-model#id) type
can be used to distinguish IDs from other strings at compile time.

## References and relationships

In Convex, you can reference a document simply by embedding its `Id` in another
document:

```ts
await ctx.db.insert("books", {
  title,
  ownerId: user._id,
});
```

You can follow references with `ctx.db.get`:

```ts
const user = await ctx.db.get(book.ownerId);
```

And [query for documents](/database/reading-data/reading-data.mdx) with a
reference:

```ts
const myBooks = await ctx.db
  .query("books")
  .filter((q) => q.eq(q.field("ownerId"), user._id))
  .collect();
```

Using `Id`s as references can allow you to build a complex data model.

## Trading off deeply nested documents vs. relationships

While it's useful that Convex supports nested objects and arrays, you should
keep documents relatively small in size. In practice, we recommend limiting
Arrays to no more than 5-10 elements and avoiding deeply nested Objects.

Instead, leverage separate tables, documents, and references to structure your
data. This will lead to better maintainability and performance as your project
grows.

## Serializing IDs

IDs are strings, which can be easily inserted into URLs or stored outside of
Convex.

You can pass an ID string from an external source (like a URL) into a Convex
function and get the corresponding object. If you're using TypeScript on the
client you can cast a string to the `Id` type:

<Snippet
  title="src/App.tsx"
  source={SerializeCall}
  highlightPatterns={[" as "]}
/>

Since this ID is coming from an external source, use an argument validator or
[`ctx.db.normalizeId`](/api/interfaces/server.GenericDatabaseReader#normalizeid)
to confirm that the ID belongs to the expected table before returning the
object.

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={SerializeExample}
  sourceJS={SerializeExample}
  highlightPatterns={["v.id"]}
/>

<StackPosts query="document IDs" />



================================================
FILE: npm-packages/docs/docs/database/pagination.mdx
================================================
---
title: "Paginated Queries"
slug: "pagination"
sidebar_position: 60
description: "Load paginated queries"
---

import Messages from "!!raw-loader!@site/../demos/pagination/convex/messages.ts";
import Download from "!!raw-loader!@site/../demos/pagination/src/download.ts";
import SimpleCall from "!!raw-loader!@site/../demos/pagination/src/_simpleListing.tsx";
import CallWithArgs from "!!raw-loader!@site/../demos/pagination/src/_listingWithArgument.tsx";

Paginated queries are [queries](/functions/query-functions.mdx) that return a
list of results in incremental pages.

This can be used to build components with "Load More" buttons or "infinite
scroll" UIs where more results are loaded as the user scrolls.

**Example:**
[Paginated Messaging App](https://github.com/get-convex/convex-demos/tree/main/pagination)

Using pagination in Convex is as simple as:

1. Writing a paginated query function that calls
   [`.paginate(paginationOpts)`](/api/interfaces/server.OrderedQuery#paginate).
2. Using the [`usePaginatedQuery`](/api/modules/react#usepaginatedquery) React
   hook.

Like other Convex queries, paginated queries are completely reactive.

## Writing paginated query functions

Convex uses cursor-based pagination. This means that paginated queries return a
string called a [`Cursor`](/api/modules/server#cursor) that represents the point
in the results that the current page ended. To load more results, you simply
call the query function again, passing in the cursor.

To build this in Convex, define a query function that:

1. Takes in a single arguments object with a `paginationOpts` property of type
   [`PaginationOptions`](/api/interfaces/server.PaginationOptions).
   - `PaginationOptions` is an object with `numItems` and `cursor` fields.
   - Use `paginationOptsValidator` exported from `"convex/server"` to
     [validate](/functions/validation.mdx) this argument
   - The arguments object may include properties as well.
2. Calls
   [`.paginate(paginationOpts)`](/api/interfaces/server.OrderedQuery#paginate)
   on a [database query](/database/reading-data/reading-data.mdx), passing in
   the `PaginationOptions` and returning its result.
   - The returned `page` in the
     [`PaginationResult`](/api/interfaces/server.PaginationResult) is an array
     of documents. You may
     [`map`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map)
     or
     [`filter`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/filter)
     it before returning it.

<TSAndJSSnippet
  sourceTS={Messages}
  sourceJS={Messages}
  title="convex/messages.ts"
  highlightPatterns={["paginationOpts"]}
  snippet="list"
/>

### Additional arguments

You can define paginated query functions that take arguments in addition to
`paginationOpts`:

<TSAndJSSnippet
  sourceTS={Messages}
  sourceJS={Messages}
  snippet="listWithExtraArg"
  title="convex/messages.ts"
/>

### Transforming results

You can apply arbitrary
[transformations](/database/reading-data/reading-data.mdx#more-complex-queries)
to the `page` property of the object returned by `paginate`, which contains the
array of documents:

<TSAndJSSnippet
  sourceTS={Messages}
  sourceJS={Messages}
  snippet="listWithTransformation"
  title="convex/messages.ts"
/>

## Paginating within React Components

To paginate within a React component, use the
[`usePaginatedQuery`](/api/modules/react#usepaginatedquery) hook. This hook
gives you a simple interface for rendering the current items and requesting
more. Internally, this hook manages the continuation cursors.

The arguments to this hook are:

- The name of the paginated query function.
- The arguments object to pass to the query function, excluding the
  `paginationOpts` (that's injected by the hook).
- An options object with the `initialNumItems` to load on the first page.

The hook returns an object with:

- `results`: An array of the currently loaded results.
- `isLoading` - Whether the hook is currently loading results.
- `status`: The status of the pagination. The possible statuses are:
  - `"LoadingFirstPage"`: The hook is loading the first page of results.
  - `"CanLoadMore"`: This query may have more items to fetch. Call `loadMore` to
    fetch another page.
  - `"LoadingMore"`: We're currently loading another page of results.
  - `"Exhausted"`: We've paginated to the end of the list.
- `loadMore(n)`: A callback to fetch more results. This will only fetch more
  results if the status is `"CanLoadMore"`.

<TSAndJSSnippet
  sourceTS={SimpleCall}
  sourceJS={SimpleCall}
  snippet="example"
  title="src/App.tsx"
  highlightPatterns={["usePaginatedQuery\\(", "api.", "{}", "initialNumItems"]}
/>

You can also pass additional arguments in the arguments object if your function
expects them:

<TSAndJSSnippet
  sourceTS={CallWithArgs}
  sourceJS={CallWithArgs}
  snippet="example"
  title="src/App.tsx"
  highlightPatterns={["author:"]}
/>

### Reactivity

Like any other Convex query functions, paginated queries are **completely
reactive**. Your React components will automatically rerender if items in your
paginated list are added, removed or changed.

One consequence of this is that **page sizes in Convex may change!** If you
request a page of 10 items and then one item is removed, this page may "shrink"
to only have 9 items. Similarly if new items are added, a page may "grow" beyond
its initial size.

## Paginating manually

If you're paginating outside of React, you can manually call your paginated
function multiple times to collect the items:

<TSAndJSSnippet title="download.ts" sourceTS={Download} sourceJS={Download} />



================================================
FILE: npm-packages/docs/docs/database/schemas.mdx
================================================
---
title: "Schemas"
sidebar_position: 5
description:
  "Schema validation keeps your Convex data neat and tidy. It also gives you
  end-to-end TypeScript type safety!"
toc_max_heading_level: 4
---

import SchemaTS from "!!raw-loader!@site/../demos/users-and-auth/convex/schema.ts";
import circularExample from "!!raw-loader!@site/../private-demos/snippets/convex/schemasCircular.ts";

A schema is a description of

1. the tables in your Convex project
2. the type of documents within your tables

While it is possible to use Convex _without_ defining a schema, adding a
`schema.ts` file will ensure that the documents in your tables are the correct
type. If you're using
[TypeScript](/understanding/best-practices/typescript.mdx), adding a schema will
also give you end-to-end type safety throughout your app.

We recommend beginning your project without a schema for rapid prototyping and
then adding a schema once you've solidified your plan. To learn more see our
[Schema Philosophy](/database/advanced/schema-philosophy.md).

**Example:**
[TypeScript and Schemas](https://github.com/get-convex/convex-demos/tree/main/typescript)

## Writing schemas

Schemas are defined in a `schema.ts` file in your `convex/` directory and look
like:

<Snippet source={SchemaTS} title="convex/schema.ts" />

This schema (which is based on our
[users and auth example](https://github.com/get-convex/convex-demos/tree/main/users-and-auth)),
has 2 tables: messages and users. Each table is defined using the
[`defineTable`](/api/modules/server#definetable) function. Within each table,
the document type is defined using the validator builder,
[`v`](/api/modules/values#v). In addition to the fields listed, Convex will also
automatically add `_id` and `_creationTime` fields. To learn more, see
[System Fields](/database/types.md#system-fields).

<Admonition type="tip" title="Generating a Schema">

While writing your schema, it can be helpful to consult the
[Convex Dashboard](/dashboard/deployments/data.md#generating-a-schema). The
"Generate Schema" button in the "Data" view suggests a schema declaration based
on the data in your tables.

</Admonition>

### Validators

The validator builder, [`v`](/api/modules/values#v) is used to define the type
of documents in each table. It has methods for each of
[Convex's types](/database/types):

```ts noDialect title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  documents: defineTable({
    id: v.id("documents"),
    string: v.string(),
    number: v.number(),
    boolean: v.boolean(),
    nestedObject: v.object({
      property: v.string(),
    }),
  }),
});
```

It additionally allows you to define unions, optional property, string literals,
and more. [Argument validation](/functions/validation.mdx) and schemas both use
the same validator builder, `v`.

#### Optional fields

You can describe optional fields by wrapping their type with `v.optional(...)`:

```typescript
defineTable({
  optionalString: v.optional(v.string()),
  optionalNumber: v.optional(v.number()),
});
```

This corresponds to marking fields as optional with `?` in TypeScript.

#### Unions

You can describe fields that could be one of multiple types using `v.union`:

```typescript
defineTable({
  stringOrNumber: v.union(v.string(), v.number()),
});
```

If your table stores multiple different types of documents, you can use
`v.union` at the top level:

```typescript
defineTable(
  v.union(
    v.object({
      kind: v.literal("StringDocument"),
      value: v.string(),
    }),
    v.object({
      kind: v.literal("NumberDocument"),
      value: v.number(),
    }),
  ),
);
```

In this schema, documents either have a `kind` of `"StringDocument"` and a
string for their `value`:

```json
{
  "kind": "StringDocument",
  "value": "abc"
}
```

or they have a `kind` of `"NumberDocument"` and a number for their `value`:

```json
{
  "kind": "NumberDocument",
  "value": 123
}
```

#### Literals

Fields that are a constant can be expressed with `v.literal`:

```typescript
defineTable({
  oneTwoOrThree: v.union(
    v.literal("one"),
    v.literal("two"),
    v.literal("three"),
  ),
});
```

#### Record objects

You can describe objects that map arbitrary keys to values with `v.record`:

```typescript
defineTable({
  simpleMapping: v.record(v.string(), v.boolean()),
});
```

You can use other types of string validators for the keys:

```typescript
import { mutation } from "./_generated/server";
import { v } from "convex/values";

export default mutation({
  args: {
    userIdToValue: v.record(v.id("users"), v.boolean()),
  },
  handler: async ({ db }, { userIdToValue }) => {
    //...
  },
});
```

Notes:

- This type corresponds to the
  [Record\<K,V\>](https://www.typescriptlang.org/docs/handbook/utility-types.html#recordkeys-type)
  type in TypeScript
- You cannot use string literals as a `record` key
- Using `v.string()` as a `record` key validator will only allow ASCII
  characters

#### Any

Fields or documents that could take on any value can be represented with
`v.any()`:

```typescript
defineTable({
  anyValue: v.any(),
});
```

This corresponds to the `any` type in TypeScript.

### Options

These options are passed as part of the
[options](/api/interfaces/server.DefineSchemaOptions) argument to
[`defineSchema`](/api/modules/server#defineschema).

#### `schemaValidation: boolean`

Whether Convex should validate at runtime that your documents match your schema.

By default, Convex will enforce that all new and existing documents match your
schema.

You can disable `schemaValidation` by passing in `schemaValidation: false`:

```typescript
defineSchema(
  {
    // Define tables here.
  },
  {
    schemaValidation: false,
  },
);
```

When `schemaValidation` is disabled, Convex will not validate that new or
existing documents match your schema. You'll still get schema-specific
TypeScript types, but there will be no validation at runtime that your documents
match those types.

#### `strictTableNameTypes: boolean`

Whether the TypeScript types should allow accessing tables not in the schema.

By default, the TypeScript table name types produced by your schema are strict.
That means that they will be a union of strings (ex. `"messages" | "users"`) and
only support accessing tables explicitly listed in your schema.

Sometimes it's useful to only define part of your schema. For example, if you
are rapidly prototyping, it could be useful to try out a new table before adding
it your `schema.ts` file.

You can disable `strictTableNameTypes` by passing in
`strictTableNameTypes: false`:

```typescript
defineSchema(
  {
    // Define tables here.
  },
  {
    strictTableNameTypes: false,
  },
);
```

When `strictTableNameTypes` is disabled, the TypeScript types will allow access
to tables not listed in the schema and their document type will be `any`.

Regardless of the value of `strictTableNameTypes`, your schema will only
validate documents in the tables listed in the schema. You can still create and
modify documents in other tables in JavaScript or on the dashboard (they just
won't be validated).

## Schema validation

Schemas are pushed automatically in
[`npx convex dev`](/cli.md#run-the-convex-dev-server) and
[`npx convex deploy`](/cli.md#deploy-convex-functions-to-production).

The first push after a schema is added or modified will validate that all
existing documents match the schema. If there are documents that fail
validation, the push will fail.

After the schema is pushed, Convex will validate that all future document
inserts and updates match the schema.

Schema validation is skipped if [`schemaValidation`](#schemavalidation-boolean)
is set to `false`.

Note that schemas only validate documents in the tables listed in the schema.
You can still create and modify documents in other tables (they just won't be
validated).

### Circular references

You might want to define a schema with circular ID references like:

```typescript title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  users: defineTable({
    preferencesId: v.id("preferences"),
  }),
  preferences: defineTable({
    userId: v.id("users"),
  }),
});
```

In this schema, documents in the `users` table contain a reference to documents
in `preferences` and vice versa.

Because schema validation enforces your schema on every `db.insert`,
`db.replace`, and `db.patch` call, creating circular references like this is not
possible.

The easiest way around this is to make one of the references nullable:

```typescript title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  users: defineTable({
    preferencesId: v.id("preferences"),
  }),
  preferences: defineTable({
    userId: v.union(v.id("users"), v.null()),
  }),
});
```

This way you can create a preferences document first, then create a user
document, then set the reference on the preferences document:

<TSAndJSSnippet
  title="convex/users.ts"
  sourceTS={circularExample}
  sourceJS={circularExample}
/>

[Let us know](/production/contact.md) if you need better support for circular
references.

## TypeScript types

Once you've defined a schema,
[`npx convex dev`](/cli.md#run-the-convex-dev-server) will produce new versions
of [`dataModel.d.ts`](/generated-api/data-model) and
[`server.d.ts`](/generated-api/server) with types based on your schema.

### `Doc<TableName>`

The [`Doc`](/generated-api/data-model#doc) TypeScript type from
[`dataModel.d.ts`](/generated-api/data-model) provides document types for all of
your tables. You can use these both when writing Convex functions and in your
React components:

```tsx noDialect title="MessageView.tsx"
import { Doc } from "../convex/_generated/dataModel";

function MessageView(props: { message: Doc<"messages"> }) {
  ...
}
```

If you need the type for a portion of a document, use the
[`Infer` type helper](/functions/validation#extracting-typescript-types).

### `query` and `mutation`

The [`query`](/generated-api/server#query) and
[`mutation`](/generated-api/server#mutation) functions in
[`server.js`](/generated-api/server) have the same API as before but now provide
a `db` with more precise types. Functions like
[`db.insert(table, document)`](/api/interfaces/server.GenericDatabaseWriter#insert)
now understand your schema. Additionally
[database queries](/database/reading-data/reading-data.mdx) will now return the
correct document type (not `any`).

<StackPosts query="schemas" />



================================================
FILE: npm-packages/docs/docs/database/types.md
================================================
---
title: "Data Types"
sidebar_position: 40
description: "Supported data types in Convex documents"
---

import ConvexValues from "@site/docs/\_convexValues.mdx";

All Convex documents are defined as Javascript objects. These objects can have
field values of any of the types below.

You can codify the shape of documents within your tables by
[defining a schema](/database/schemas.mdx).

## Convex values

<ConvexValues />

## System fields

Every document in Convex has two automatically-generated system fields:

- `_id`: The [document ID](/database/document-ids.mdx) of the document.
- `_creationTime`: The time this document was created, in milliseconds since the
  Unix epoch.

## Limits

Convex values must be less than 1MB in total size. This is an approximate limit
for now, but if you're running into these limits and would like a more precise
method to calculate a document's size,
[reach out to us](https://convex.dev/community). Documents can have nested
values, either objects or arrays that contain other Convex types. Convex types
can have at most 16 levels of nesting, and the cumulative size of a nested tree
of values must be under the 1MB limit.

Table names may contain alphanumeric characters ("a" to "z", "A" to "Z", and "0"
to "9") and underscores ("\_"), and they cannot start with an underscore.

For information on other limits, see [here](/production/state/limits.mdx).

If any of these limits don't work for you,
[let us know](https://convex.dev/community)!

## Working with `undefined`

The TypeScript value `undefined` is not a valid Convex value, so it cannot be
used in Convex function arguments or return values, or in stored documents.

1. Objects/records with `undefined` values are the same as if the field were
   missing: `{a: undefined}` is transformed into `{}` when passed to a function
   or stored in the database. You can think of Convex function calls and the
   Convex database as serializing the data with `JSON.stringify`, which
   similarly removes `undefined` values.
2. Validators for object fields can use `v.optional(...)` to indicate that the
   field might not be present.
   - If an object's field "a" is missing, i.e. `const obj = {};`, then
     `obj.a === undefined`. This is a property of TypeScript/JavaScript, not
     specific to Convex.
3. You can use `undefined` in filters and index queries, and it will match
   documents that do not have the field. i.e.
   `.withIndex("by_a", q=>q.eq("a", undefined))` matches document `{}` and
   `{b: 1}`, but not `{a: 1}` or `{a: null, b: 1}`.
   - In Convex's ordering scheme, `undefined < null < all other values`, so you
     can match documents that _have_ a field via `q.gte("a", null as any)` or
     `q.gt("a", undefined)`.
4. There is exactly one case where `{a: undefined}` is different from `{}`: when
   passed to `ctx.db.patch`. Passing `{a: undefined}` removes the field "a" from
   the document, while passing `{}` does not change the field "a". See
   [Updating existing documents](/database/writing-data.mdx#updating-existing-documents).
5. Since `undefined` gets stripped from function arguments but has meaning in
   `ctx.db.patch`, there are some tricks to pass patch's argument from the
   client.
   - If the client passing `args={}` (or `args={a: undefined}` which is
     equivalent) should leave the field "a" unchanged, use
     `ctx.db.patch(id, args)`.
   - If the client passing `args={}` should remove the field "a", use
     `ctx.db.patch(id, {a: undefined, ...args})`.
   - If the client passing `args={}` should leave the field "a" unchanged and
     `args={a: null}` should remove it, you could do
     ```ts
     if (args.a === null) {
       args.a = undefined;
     }
     await ctx.db.patch(id, args);
     ```
6. Functions that return a plain `undefined`/`void` are treated as if they
   returned `null`.
7. Arrays containing `undefined` values, like `[undefined]`, throw an error when
   used as Convex values.

If you would prefer to avoid the special behaviors of `undefined`, you can use
`null` instead, which _is_ a valid Convex value.

## Working with dates and times

Convex does not have a special data type for working with dates and times. How
you store dates depends on the needs of your application:

1. If you only care about a point in time, you can store a
   [UTC timestamp](https://en.wikipedia.org/wiki/Unix_time). We recommend
   following the `_creationTime` field example, which stores the timestamp as a
   `number` in milliseconds. In your functions and on the client you can create
   a JavaScript `Date` by passing the timestamp to its constructor:
   `new Date(timeInMsSinceEpoch)`. You can then print the date and time in the
   desired time zone (such as your user's machine's configured time zone).
   - To get the current UTC timestamp in your function and store it in the
     database, use `Date.now()`
2. If you care about a calendar date or a specific clock time, such as when
   implementing a booking app, you should store the actual date and/or time as a
   string. If your app supports multiple timezones you should store the timezone
   as well. [ISO8601](https://en.wikipedia.org/wiki/ISO_8601) is a common format
   for storing dates and times together in a single string like
   `"2024-03-21T14:37:15Z"`. If your users can choose a specific time zone you
   should probably store it in a separate `string` field, usually using the
   [IANA time zone name](https://en.wikipedia.org/wiki/Tz_database#Names_of_time_zones)
   (although you could concatenate the two fields with unique character like
   `"|"`).

For more sophisticated printing (formatting) and manipulation of dates and times
use one of the popular JavaScript libraries: [date-fns](https://date-fns.org/),
[Day.js](https://day.js.org/), [Luxon](https://moment.github.io/luxon/) or
[Moment.js](https://momentjs.com/).



================================================
FILE: npm-packages/docs/docs/database/writing-data.mdx
================================================
---
title: "Writing Data"
sidebar_position: 4
description: "Insert, update, and delete data in Convex database tables"
---

import insertExample from "!!raw-loader!@site/../private-demos/snippets/convex/writingDataInsert.ts";
import patchExample from "!!raw-loader!@site/../private-demos/snippets/convex/writingDataPatch.ts";
import replaceExample from "!!raw-loader!@site/../private-demos/snippets/convex/writingDataReplace.ts";
import deleteExample from "!!raw-loader!@site/../private-demos/snippets/convex/writingDataDelete.ts";
import { ComponentCardList } from "@site/src/components/ComponentCard";

[Mutations](/functions/mutation-functions.mdx) can insert, update, and remove
data from database tables.

## Inserting new documents

You can create new documents in the database with the
[`db.insert`](/api/interfaces/server.GenericDatabaseWriter#insert) method:

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={insertExample}
  sourceJS={insertExample}
  highlightPatterns={["db.insert"]}
/>

The second argument to `db.insert` is a JavaScript object with data for the new
document.

The same types of values that can be passed into and returned from
[queries](/functions/query-functions.mdx) and
[mutations](/functions/mutation-functions.mdx) can be written into the database.
See [Data Types](/database/types.md) for the full list of supported types.

The `insert` method returns a globally unique ID for the newly inserted
document.

## Updating existing documents

Given an existing document ID the document can be updated using the following
methods:

1. The [`db.patch`](/api/interfaces/server.GenericDatabaseWriter#patch) method
   will patch an existing document, shallow merging it with the given partial
   document. New fields are added. Existing fields are overwritten. Fields set
   to `undefined` are removed.

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={patchExample}
  sourceJS={patchExample}
  highlightPatterns={["db.patch"]}
/>

2. The [`db.replace`](/api/interfaces/server.GenericDatabaseWriter#replace)
   method will replace the existing document entirely, potentially removing
   existing fields:

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={replaceExample}
  sourceJS={replaceExample}
  highlightPatterns={["db.replace"]}
/>

## Deleting documents

Given an existing document ID the document can be removed from the table with
the [`db.delete`](/api/interfaces/server.GenericDatabaseWriter#delete) method.

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={deleteExample}
  sourceJS={deleteExample}
  highlightPatterns={["db.delete"]}
/>

## Bulk inserts or updates

If you are used to SQL you might be looking for some sort of bulk insert or bulk
update statement. In Convex the entire `mutation` function is automatically a
single transaction.

You can just insert or update in a loop in the mutation function. Convex queues
up all database changes in the function and executes them all in a single
transaction when the function ends, leading to a single efficient change to the
database.

````typescript
/**
 * Bulk insert multiple products into the database.
 *
 * Equivalent to the SQL:
 * ```sql
 * INSERT INTO products (product_id, product_name, category, price, in_stock)
 * VALUES
 *     ('Laptop Pro', 'Electronics', 1299.99, true),
 *     ('Wireless Mouse', 'Electronics', 24.95, true),
 *     ('Ergonomic Keyboard', 'Electronics', 89.50, true),
 *     ('Ultra HD Monitor', 'Electronics', 349.99, false),
 *     ('Wireless Headphones', 'Audio', 179.99, true);
 * ```
 */
export const bulkInsertProducts = mutation({
  args: {
    products: v.array(
      v.object({
        product_name: v.string(),
        category: v.string(),
        price: v.number(),
        in_stock: v.boolean(),
      }),
    ),
  },
  handler: async (ctx, args) => {
    const { products } = args;

    // Insert in a loop. This is efficient because Convex queues all the changes
    // to be executed in a single transaction when the mutation ends.
    for (const product of products) {
      const id = await ctx.db.insert("products", {
        product_name: product.product_name,
        category: product.category,
        price: product.price,
        in_stock: product.in_stock,
      });
    }
  },
});
````

## Migrations

Database migrations are done through the migration component. The component is
designed to run online migrations to safely evolve your database schema over
time. It allows you to resume from failures, and validate changes with dry runs.

<ComponentCardList
  items={[
    {
      title: "Migrations",
      description: "Framework for long running data migrations of live data.",
      href: "https://www.convex.dev/components/migrations",
    },
  ]}
/>

## Write performance and limits

To prevent accidental writes of large amounts of records, queries and mutations
enforce limits detailed [here](/production/state/limits.mdx#transactions).



================================================
FILE: npm-packages/docs/docs/database/advanced/_category_.json
================================================
{
  "label": "Advanced",
  "position": 100
}



================================================
FILE: npm-packages/docs/docs/database/advanced/occ.md
================================================
---
title: "OCC and Atomicity"
slug: "occ"
hidden: false
sidebar_position: 500
todo: Push under mutations, or inline, or kill (move to Stack)
description:
  "Optimistic concurrency control and transaction atomicity in Convex"
---

In [Queries](/functions/query-functions.mdx), we mentioned that determinism as
important in the way optimistic concurrency control (OCC) was used within
Convex. In this section, we'll dive much deeper into _why_.

## Convex Financial, Inc.

Imagine that you're building a banking app, and therefore your databases stores
accounts with balances. You want your users to be able to give each other money,
so you write a mutation function that transfers funds from one user's account to
another.

One run of that transaction might read Alice's account balance, and then Bob's.
You then propose to deduct $5 from Alice's account and increase Bob's balance by
the same $5.

Here's our pseudocode:

```
$14 <- READ Alice
$11 <- READ Bob
WRITE Alice $9
WRITE Bob $16
```

This ledger balance transfer is a classic database scenario that requires a
guarantee that these write operations will only apply together. It is a really
bad thing if only one operation succeeds!

```
$14 <- READ Alice
$11 <- READ Bob
WRITE Alice $9
*crash* // $5 lost from your bank
```

You need a guarantee that this can never happen. You require transaction
atomicity, and Convex provides it.

The problem of data correctness is much deeper. Concurrent transactions that
read and edit the same records can create _data races_.

In the case of our app it's entirely possible that someone deducts Alice's
balance right after we read it. Maybe she bought a Coke Zero at the airport with
her debit card for $3.

```
$5 Transfer                           $3 Debit Card Charge
----------------------------------------------------------
$14 <- READ Alice
$11 <- READ Bob
                                        $14 <- READ Alice
                                        WRITE Alice $11
WRITE Alice $9 // Free coke!
WRITE Bob $16
```

Clearly, we need to prevent these types of data races from happening. We need a
way to handle these concurrent conflicts. Generally, there are two common
approaches.

Most traditional databases choose a _pessimistic locking_ strategy. (Pessimism
in this case means the strategy assumes conflict will happen ahead of time so
seeks to prevent it.) With pessimistic locking, you first need to acquire a lock
on Alice's record, and then acquire a lock on Bob's record. Then you can proceed
to conduct your transaction, knowing that any other transaction that needed to
touch those records will wait until you are done and all your writes are
committed.

After decades of experience, the drawbacks of pessimistic locking are well
understood and undeniable. The biggest limitation arises from real-life networks
and computers being inherently unreliable. If the lock holder goes missing for
whatever reason half way through its transaction, everyone else that wants to
modify any of those records is waiting indefinitely. Not good!

Optimistic concurrency control is, as the name states, optimistic. It assumes
the transaction will succeed and doesn't worry about locking anything ahead of
time. Very brash! How can it be so sure?

It does this by treating the transaction as a _declarative proposal_ to write
records on the basis of any read record versions (the "read set"). At the end of
the transaction, the writes all commit if every version in the read set is still
the latest version of that record. This means no concurrent conflict occurred.

Now using our version read set, let's see how OCC would have prevented the
soda-catastrophe above:

```
$5 Transfer                           $3 Debit Card Charge
----------------------------------------------------------
(v1, $14) <- READ Alice
(v7, $11) <- READ Bob
                                        (v1, $14) <- READ Alice
                                        WRITE Alice $11
                                        IF Alice.v = v1

WRITE Alice = $9, Bob = $16
    IF Alice.v = v1, Bob.v = v7 // Fails! Alice is = v2
```

This is akin to being unable to push your Git repository because you're not at
HEAD. We all know in that circumstance, we need to pull, and rebase or merge,
etc.

## When OCC loses, determinism wins

A naive optimistic concurrency control solution would be to solve this the same
way that Git does: require the user/application to resolve the conflict and
determine if it is safe to retry.

In Convex, however, we don't need to do that. We know the transaction is
deterministic. It didn't charge money to Stripe, it didn't write a permanent
value out to the filesystem. It had no effect at all other than proposing some
atomic changes to Convex tables that were not applied.

The determinism means that we can simply re-run the transaction; you never need
to worry about temporary data races. We can run several retries if necessary
until we succeed to execute the transaction without any conflicts.

<Admonition type="tip">

In fact, the Git analogy stays very apt. An OCC conflict means we cannot push
because our HEAD is out of date, so we need to rebase our changes and try again.
And determinism is what guarantees there is never a "merge conflict", so (unlike
with Git) this rebase operation will always eventually succeed without developer
intervention.

</Admonition>

## Snapshot Isolation vs Serializability

It is common for optimistic multi-version concurrency control databases to
provide a guarantee of
[snapshot isolation](https://en.wikipedia.org/wiki/Snapshot_isolation). This
[isolation level](<https://en.wikipedia.org/wiki/Isolation_(database_systems)>)
provides the illusion that all transactions execute on an atomic snapshot of the
data but it is vulnerable to
[anomalies](https://en.wikipedia.org/wiki/Snapshot_isolation#Definition) where
certain combinations of concurrent transactions can yield incorrect results. The
implementation of optimistic concurrency control in Convex instead provides true
[serializability](https://en.wikipedia.org/wiki/Serializability) and will yield
correct results regardless of what transactions are issued concurrently.

## No need to think about this

The beauty of this approach is that you can simply write your mutation functions
as if they will _always succeed_, and always be guaranteed to be atomic.

Aside from sheer curiosity about how Convex works, day to day there's no need to
worry about conflicts, locking, or atomicity when you make changes to your
tables and documents. The "obvious way" to write your mutation functions will
just work.



================================================
FILE: npm-packages/docs/docs/database/advanced/schema-philosophy.md
================================================
---
title: Schema Philosophy
sidebar_position: 450
description: "Convex schema design philosophy and best practices"
---

With Convex there is no need to write any `CREATE TABLE` statements, or think
through your stored table structure ahead of time so you can name your field and
types. You simply put your objects into Convex and keep building your app!

However, moving fast early can be problematic later. "Was that field a number or
a string? I think I changed it when I fixed that one bug?"

Storage systems which are too permissive can sometimes become liabilities as
your system matures and you want to be able to reason assuredly about exactly
what data is in your system.

The good news is Convex is always typed. It's just implicitly typed! When you
submit a document to Convex, tracks all the types of all the fields in your
document. You can go to your [dashboard](/dashboard.md) and view the inferred
schema of any table to understand what you've ended up with.

"What about that field I changed from a string to a number?" Convex can handle
this too. Convex will track those changes, in this case the field is a union
like `v.union(v.number(), v.string())`. That way even when you change your mind
about your documents fields and types, Convex has your back.

Once you are ready to formalize your schema, you can define it using our
[schema builder](/database/schemas.mdx) to enable schema validation and generate
types based on it.



================================================
FILE: npm-packages/docs/docs/database/advanced/system-tables.mdx
================================================
---
title: "System Tables"
sidebar_position: 1
---

System tables enable read-only access to metadata for built-in Convex features.
Currently there are two system tables exposed:

- `"_scheduled_functions"` table contains metadata for
  [scheduled functions](/scheduling/scheduled-functions.mdx#retrieving-scheduled-function-status)
- `"_storage"` table contains metadata for
  [stored files](/file-storage/file-metadata.mdx)

You can read data from system tables using the `db.system.get` and
`db.system.query` methods, which work the same as the standard `db.get` and
`db.query` methods. Queries reading from system tables are reactive and realtime
just like queries reading from all other tables, and pagination can be used to
enumerate all documents even when there are too many to read in a single query.



================================================
FILE: npm-packages/docs/docs/database/import-export/export.mdx
================================================
---
title: "Data Export"
sidebar_label: "Data Export"
description: "Export your data out of Convex"
sidebar_position: 168
---

You can export your data from Convex by
[taking a backup](/database/backup-restore) and downloading it as a zip file.

Alternatively, you can export the same data with the
[command line](/cli.md#export-data-to-a-file):

```sh
npx convex export --path ~/Downloads
```



================================================
FILE: npm-packages/docs/docs/database/import-export/import-export.mdx
================================================
---
title: "Data Import & Export"
sidebar_position: 90
description:
  "Import data from existing sources and export data to external systems"
---

If you're bootstrapping your app from existing data, Convex provides three ways
to get the data in:

- Import from csv/json into a single table via the
  [CLI](/database/import-export/import.mdx#single-table-import).
- Restore from a backup via the [dashboard](/database/backup-restore) or
  [CLI](/database/import-export/import.mdx#restore-data-from-a-backup-zip-file).
- [Streaming import](/production/integrations/streaming-import-export.md) from
  any existing database via Airbyte destination connector.

You can export data from Convex in two ways.

- Download a backup as a zip from the [dashboard](/database/backup-restore).
- Set up [streaming export](/production/integrations/streaming-import-export.md)
  to any external database via Fivetran or Airbyte. Great for connecting to a
  custom BI setup (eg [Snowflake](https://www.snowflake.com/),
  [Databricks](https://www.databricks.com), or
  [BigQuery](https://cloud.google.com/bigquery)):

<BetaAdmonition feature="Data Import & Export" verb="is" />



================================================
FILE: npm-packages/docs/docs/database/import-export/import.mdx
================================================
---
title: "Data Import"
sidebar_label: "Data Import"
description: "Import data into Convex"
sidebar_position: 169
---

You can import data into Convex from a local file using the command line.

```sh
npx convex import
```

<BetaAdmonition feature="Data import" verb="is" />

Use `--help` to see all options. The most common flows are described here.

## Single table import

```sh
npx convex import --table <tableName> <path>
```

Import a CSV, JSON, or JSONLines file into a Convex table.

- `.csv` files must have a header, and each row's entries are interpreted either
  as a (floating point) number or a string.
- `.jsonl` files must have a JSON object per line.
- `.json` files must be an array of JSON objects.
  - JSON arrays have a size limit of 8MiB. To import more data, use CSV or
    JSONLines. You can convert json to jsonl with a command like
    `jq -c '.[]' data.json > data.jsonl`

Imports into a table with existing data will fail by default, but you can
specify `--append` to append the imported rows to the table or `--replace` to
replace existing data in the table with your import.

The default is to import into your dev deployment. Use `--prod` to import to
your production deployment or `--preview-name` to import into a preview
deployment.

## Restore data from a backup ZIP file

```sh
npx convex import <path>.zip
```

Import from a [Backup](/database/backup-restore) into a Convex deployment, where
the backup is a ZIP file that has been downloaded on the dashboard. Documents
will retain their `_id` and `_creationTime` fields so references between tables
are maintained.

Imports where tables have existing data will fail by default, but you can
specify `--replace` to replace existing data in tables mentioned in the ZIP
file.

## Use cases

1. Seed dev deployments with sample data.

```sh
# full backup - exported from prod or another dev deployment.
npx convex import seed_data.zip

# Import single table from jsonl/csv
npx convex import --table <table name> data.jsonl
```

2. Restore a deployment from a [backup](/database/backup-restore)
   programmatically. Download a backup, and restore from this backup if needed.

```sh
npx convex import --prod --replace backup.zip
```

3. Seed preview deployments with sample data, exported from prod, dev, or
   another preview deployment. Example for Vercel, seeding data from
   `seed_data.zip` committed in the root of the repo.

```sh
npx convex deploy --cmd 'npm run build' &&
if [ "$VERCEL_ENV" == "preview" ]; then
npx convex import --preview-name "$VERCEL_GIT_COMMIT_REF" seed_data.zip;
fi
```

4. Clear a table efficiently with an empty import.

```sh
touch empty_file.jsonl
npx convex import --replace --table <tableNameToClear> empty_file.jsonl
```

## Features

- Data import is the only way to create documents with pre-existing `_id` and
  `_creationTime` fields.
  - The `_id` field must match Convex's ID format.
  - If `_id` or `_creationTime` are not provided, new values are chosen during
    import.
- Data import creates and replaces tables atomically (except when using
  `--append`).
  - Queries and mutations will not view intermediate states where partial data
    is imported.
  - Indexes and schemas will work on the new data without needing time for
    re-backfilling or re-validating.
- Data import only affects tables that are mentioned in the import, either by
  `--table` or as entries in the ZIP file.
- While JSON and JSONLines can import arbitrary JSON values, ZIP imports can
  additionally import other Convex values: Int64, Bytes, etc. Types are
  preserved in the ZIP file through the `generated_schema.jsonl` file.
- Data import of ZIP files that include [file storage](/file-storage) import the
  files and preserve [`_storage`](/database/advanced/system-tables.mdx)
  documents, including their `_id`, `_creationTime`, and `contentType` fields.

## Warnings

- [Streaming Export](/production/integrations/streaming-import-export.md)
  (Fivetran or Airbyte) does not handle data imports or backup restorations,
  similar to table deletion and creation and some schema changes. We recommend
  resetting streaming export sync after a restore or a data import.
- Avoid changing the ZIP file between downloading it from Data Export and
  importing it with `npx convex import`. Some manual changes of the ZIP file may
  be possible, but remain undocumented. Please share your use case and check
  with the Convex team in [Discord](https://convex.dev/community).
- Data import is not always supported when importing into a deployment that was
  created before Convex version 1.7.
  - The import may work, especially when importing a ZIP backup from a
    deployment created around the same time as the target deployment. As a
    special case, you can always restore from backups from its own deployment.
  - Reach out in [Discord](https://convex.dev/community) if you encounter
    issues, as there may be a workaround.

Data import uses database bandwidth to write all documents, and file bandwidth
if the export includes file storage. You can observe this bandwidth in the
[usage dashboard](https://dashboard.convex.dev/team/settings/usage) as function
name `_cli/import` and associated cost in the
[limits docs](/production/state/limits#database).



================================================
FILE: npm-packages/docs/docs/database/reading-data/filters.mdx
================================================
---
title: "Filters"
sidebar_position: 200
description: "Filter documents in Convex queries"
---

# Filtering

The [`filter`](/api/interfaces/server.Query#filter) method allows you to
restrict the documents that your document query returns. This method takes a
filter constructed by [`FilterBuilder`](/api/interfaces/server.FilterBuilder)
and will only select documents that match.

The examples below demonstrate some of the common uses of `filter`. You can see
the full list of available filtering methods
[in the reference docs](/api/interfaces/server.FilterBuilder).

If you need to filter to documents containing some keywords, use a
[search query](/search/search.mdx).

<Admonition type="caution" title="Use indexes instead">
  Filters effectively loop over your table looking for documents that match.
  This can be slow or cause your function to hit a
  [limit](/production/state/limits.mdx) when your table has thousands of rows.
  For faster more database efficient queries use [indexes
  instead](/database/reading-data/indexes/indexes.md).
</Admonition>

### Equality conditions

This document query finds documents in the `users` table where
`doc.name === "Alex"`:

```ts
// Get all users named "Alex".
const usersNamedAlex = await ctx.db
  .query("users")
  .filter((q) => q.eq(q.field("name"), "Alex"))
  .collect();
```

Here `q` is the [`FilterBuilder`](/api/interfaces/server.FilterBuilder) utility
object. It contains methods for all of our supported filter operators.

This filter will run on all documents in the table. For each document,
`q.field("name")` evaluates to the `name` property. Then `q.eq` checks if this
property is equal to `"Alex"`.

If your query references a field that is missing from a given document then that
field will be considered to have the value `undefined`.

### Comparisons

Filters can also be used to compare fields against values. This document query
finds documents where `doc.age >= 18`:

```ts
// Get all users with an age of 18 or higher.
const adults = await ctx.db
  .query("users")
  .filter((q) => q.gte(q.field("age"), 18))
  .collect();
```

Here the `q.gte` operator checks if the first argument (`doc.age`) is greater
than or equal to the second (`18`).

Here's the full list of comparisons:

| Operator      | Equivalent TypeScript |
| ------------- | --------------------- |
| `q.eq(l, r)`  | `l === r`             |
| `q.neq(l, r)` | `l !== r`             |
| `q.lt(l, r)`  | `l < r`               |
| `q.lte(l, r)` | `l <= r`              |
| `q.gt(l, r)`  | `l > r`               |
| `q.gte(l, r)` | `l >= r`              |

### Arithmetic

You can also include basic arithmetic in your queries. This document query finds
documents in the `carpets` table where `doc.height * doc.width > 100`:

```ts
// Get all carpets that have an area of over 100.
const largeCarpets = await ctx.db
  .query("carpets")
  .filter((q) => q.gt(q.mul(q.field("height"), q.field("width")), 100))
  .collect();
```

Here's the full list of arithmetic operators:

| Operator      | Equivalent TypeScript |
| ------------- | --------------------- |
| `q.add(l, r)` | `l + r`               |
| `q.sub(l, r)` | `l - r`               |
| `q.mul(l, r)` | `l * r`               |
| `q.div(l, r)` | `l / r`               |
| `q.mod(l, r)` | `l % r`               |
| `q.neg(x)`    | `-x`                  |

### Combining operators

You can construct more complex filters using methods like `q.and`, `q.or`, and
`q.not`. This document query finds documents where
`doc.name === "Alex" && doc.age >= 18`:

```ts
// Get all users named "Alex" whose age is at least 18.
const adultAlexes = await ctx.db
  .query("users")
  .filter((q) =>
    q.and(q.eq(q.field("name"), "Alex"), q.gte(q.field("age"), 18)),
  )
  .collect();
```

Here is a query that finds all users where
`doc.name === "Alex" || doc.name === "Emma"`:

```ts
// Get all users named "Alex" or "Emma".
const usersNamedAlexOrEmma = await ctx.db
  .query("users")
  .filter((q) =>
    q.or(q.eq(q.field("name"), "Alex"), q.eq(q.field("name"), "Emma")),
  )
  .collect();
```

## Advanced filtering techniques

Sometimes the filter syntax is is not expressive enough. For example you may
want to collect all posts that have a tag. Your schema for the posts looks like
this:

```ts
export default defineSchema({
  posts: defineTable({
    body: v.string(),
    tags: v.array(v.string()),
  }),
});
```

One way to solve is by applying the filter on the result of the `collect()`
call. This is just filtering a JavaScript array:

```ts
export const postsWithTag = query({
  args: { tag: v.string() },
  handler: async (ctx, args) => {
    const allPosts = await ctx.db.query("posts").collect();
    return allPosts.filter((post) => post.tags.includes(args.tag));
  },
});
```

But this requires reading the whole table first. If you want to just get the
first result that matches, reading the whole table could be very inefficient.
Instead you may want to use the JavaScript
[`for await...of`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of)
syntax to loop through the table one document at a time:

```ts
export const firstPostWithTag = query({
  args: { tag: v.string() },
  handler: (ctx, args) => {
    for await (const post of db.query("posts")) {
      if (post.tags.includes(args.tag)) {
        return post;
      }
    }
  },
});
```

This works because Convex queries are
[JavaScript iterables](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols).

Even with this optimization you are still just looping over the table to find
the first post that matches and may hit your function limits. Using indexes is
still the way to go. You can read a
[detailed discussion of how to handle tags with indexes](https://stack.convex.dev/complex-filters-in-convex#optimize-with-indexes).

## Querying performance and limits

Most of the example document queries above can lead to a _full table scan_. That
is, for the document query to return the requested results, it might need to
walk over every single document in the table.

Take this simple example:

```ts
const tasks = await ctx.db.query("tasks").take(5);
```

This document query will not scan more than 5 documents.

On the other hand, this document query:

```ts
const tasks = await ctx.db
  .query("tasks")
  .filter((q) => q.eq(q.field("isCompleted"), true))
  .first();
```

might need to walk over every single document in the `"tasks"` table just to
find the first one with `isCompleted: true`.

If a table has more than a few thousand documents, you should use
[indexes](/database/reading-data/indexes/indexes.md) to improve your document
query performance. Otherwise, you may run into our enforced limits, detailed in
[Read/write limit errors](/functions/error-handling/error-handling.mdx#readwrite-limit-errors).

For information on other limits, see [Limits](/production/state/limits.mdx).



================================================
FILE: npm-packages/docs/docs/database/reading-data/reading-data.mdx
================================================
---
title: "Reading Data"
sidebar_position: 3
description: "Query and read data from Convex database tables"
---

import getExample from "!!raw-loader!@site/../private-demos/snippets/convex/readingDataDbGet.ts";
import queryExample from "!!raw-loader!@site/../private-demos/snippets/convex/readingDataDbQuery.ts";
import averageExample from "!!raw-loader!@site/../private-demos/snippets/convex/readingDataAverage.ts";
import groupByExampleTS from "!!raw-loader!@site/../private-demos/snippets/convex/readingDataGroupByTS.ts";
import groupByExampleJS from "!!raw-loader!@site/../private-demos/snippets/convex/readingDataGroupByJS.js";
import joinExample from "!!raw-loader!@site/../private-demos/snippets/convex/readingDataJoin.ts";

[Query](/functions/query-functions.mdx) and
[mutation](/functions/mutation-functions.mdx) functions can read data from
database tables using _document ids_ and _document queries_.

## Reading a single document

Given a single document's id you can read its data with the
[`db.get`](/api/interfaces/server.GenericDatabaseReader#get) method:

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={getExample}
  sourceJS={getExample}
  highlightPatterns={["db.get"]}
/>

**Note**: You should use the `v.id` validator like in the example above to make
sure you are not exposing data from tables other than the ones you intended.

## Querying documents

Document queries always begin by choosing the table to query with the
[`db.query`](/api/interfaces/server.GenericDatabaseReader#query) method:

<TSAndJSSnippet
  title="convex/tasks.ts"
  sourceTS={queryExample}
  sourceJS={queryExample}
  highlightPatterns={["db.query"]}
/>

Then you can:

1. filter
2. order
3. and `await` the results

We'll see how this works in the examples below.

## Filtering your query

The best way to filter in Convex is to use indexes. Indexes build a special
internal structure in your database to speed up lookups.

There are two steps to using indexes:

1. Define the index in your `convex/schema.ts` file.
2. Query via the `withIndex()` syntax.

### 1. Define the index

If you aren't familiar with how to create a Convex schema, read the
[schema doc](/database/schemas.mdx).

Let’s assume you’re building a chat app and want to get all messages in a
particular channel. You can define a new index called `by_channel` on the
`messages` table by using the `.index()` method in your schema.

```ts noDialect title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

// Define a messages table with an index.
export default defineSchema({
  messages: defineTable({
    channel: v.id("channels"),
    body: v.string(),
    user: v.id("users"),
    // highlight-next-line
  }).index("by_channel", ["channel"]),
});
```

### 2. Filter a query with an index

In your query function, you can now filter your `messages` table by using the
`by_channel` index.

```ts
const messages = await ctx.db
  .query("messages")
  // highlight-next-line
  .withIndex("by_channel", (q) => q.eq("channel", channel))
  .collect();
```

In Convex, you must explicitly use the `withIndex()` syntax to ensure your
database uses the index. This differs from a more traditional SQL database,
where the database implicitly chooses to use an index based on heuristics. The
Convex approach leads to fewer surprises in the long run.

You can create an index across multiple fields at once, query a specific range
of data, and change the order of your query result.
[Read the complete index documentation](/database/reading-data/indexes/indexes.md)
to learn more.

Convex also supports a slower filtering mechanism that effectively loops through
the table to match the filter. This can be useful if you know your table will be
small (low thousands of rows), you're prototyping, or you want to filter an
index query further. You can read more about filters
[here](/database/reading-data/filters.mdx).

## Ordering

By default Convex always returns documents ordered by
[`_creationTime`](/database/types.md#system-fields).

You can use [`.order("asc" | "desc")`](/api/interfaces/server.Query#order) to
pick whether the order is ascending or descending. If the order isn't specified,
it defaults to ascending.

```ts
// Get all messages, oldest to newest.
const messages = await ctx.db.query("messages").order("asc").collect();
```

```ts
// Get all messages, newest to oldest.
const messages = await ctx.db.query("messages").order("desc").collect();
```

If you need to sort on a field other than `_creationTime` and your document
query returns a small number of documents (on the order of hundreds rather than
thousands of documents), consider sorting in Javascript:

```ts
// Get top 10 most liked messages, assuming messages is a fairly small table:
const messages = await ctx.db.query("messages").collect();
const topTenMostLikedMessages = recentMessages
  .sort((a, b) => b.likes - a.likes)
  .slice(0, 10);
```

For document queries that return larger numbers of documents, you'll want to use
an [index](/database/reading-data/indexes/indexes.md) to improve the
performance. Document queries that use indexes will be
[ordered based on the columns in the index](/database/reading-data/indexes/indexes.md#sorting-with-indexes)
and can avoid slow table scans.

```ts
// Get the top 20 most liked messages of all time, using the "by_likes" index.
const messages = await ctx.db
  .query("messages")
  .withIndex("by_likes")
  .order("desc")
  .take(20);
```

See [Limits](/database/reading-data/indexes/indexes.md#limits) for details.

### Ordering of different types of values

A single field can have values of any [Convex type](/database/types.md). When
there are values of different types in an indexed field, their ascending order
is as follows:

No value set&nbsp;(`undefined`) < Null&nbsp;(`null`) < Int64&nbsp;(`bigint`) <
Float64 (`number`) < Boolean&nbsp;(`boolean`) < String&nbsp;(`string`) <
Bytes&nbsp;(`ArrayBuffer`) < Array&nbsp;(`Array`) < Object&nbsp;(`Object`)

The same ordering is used by the filtering comparison operators `q.lt()`,
`q.lte()`, `q.gt()` and `q.gte()`.

## Retrieving results

Most of our previous examples have ended the document query with the
[`.collect()`](/api/interfaces/server.Query#collect) method, which returns all
the documents that match your filters. Here are the other options for retrieving
results.

### Taking `n` results

[`.take(n)`](/api/interfaces/server.Query#take) selects only the first `n`
results that match your query.

```ts
const users = await ctx.db.query("users").take(5);
```

### Finding the first result

[`.first()`](/api/interfaces/server.Query#first) selects the first document that
matches your query and returns `null` if no documents were found.

```ts
// We expect only one user with that email address.
const userOrNull = await ctx.db
  .query("users")
  .withIndex("by_email", (q) => q.eq("email", "test@example.com"))
  .first();
```

### Using a unique result

[`.unique()`](/api/interfaces/server.Query#unique) selects the single document
from your query or returns `null` if no documents were found. If there are
multiple results it will throw an exception.

```ts
// Our counter table only has one document.
const counterOrNull = await ctx.db.query("counter").unique();
```

### Loading a page of results

[`.paginate(opts)`](/api/interfaces/server.OrderedQuery#paginate) loads a page
of results and returns a [`Cursor`](/api/modules/server#cursor) for loading
additional results.

See [Paginated Queries](/database/pagination.mdx) to learn more.

## More complex queries

Convex prefers to have a few, simple ways to walk through and select documents
from tables. In Convex, there is no specific query language for complex logic
like a join, an aggregation, or a group by.

Instead, you can write the complex logic in Javascript! Convex guarantees that
the results will be consistent.

### Join

Table join might look like:

<TSAndJSSnippet
  title="convex/events.ts"
  sourceTS={joinExample}
  sourceJS={joinExample}
/>

### Aggregation

Here's an example of computing an average:

<TSAndJSSnippet
  title="convex/purchases.ts"
  sourceTS={averageExample}
  sourceJS={averageExample}
/>

> If you need more scalable aggregate options (for example to handle frequent
> updates or large tables), consider using the
> [Sharded Counter](https://www.convex.dev/components/sharded-counter) or
> [Aggregate](https://www.convex.dev/components/aggregate) components. These
> components can help you handle high-throughput counters, sums, or computations
> without looping through the whole table.

### Group by

Here's an example of grouping and counting:

<TSAndJSSnippet
  title="convex/purchases.ts"
  sourceTS={groupByExampleTS}
  sourceJS={groupByExampleJS}
/>

## Explore the syntax on the dashboard

You can try out the syntax described above directly from the dashboard by
[writing a custom test query](/dashboard/deployments/data.md#writing-custom-queries).



================================================
FILE: npm-packages/docs/docs/database/reading-data/indexes/indexes-and-query-perf.md
================================================
---
sidebar_label: "Indexes and Query Performance"
title: "Introduction to Indexes and Query Performance"
sidebar_position: 100
description: "Learn the effects of indexes on query performance"
---

How do I ensure my Convex
[database queries](/database/reading-data/reading-data.mdx) are fast and
efficient? When should I define an
[index](/database/reading-data/indexes/indexes.md)? What is an index?

This document explains how you should think about query performance in Convex by
describing a simplified model of how queries and indexes function.

If you already have a strong understanding of database queries and indexes you
can jump straight to the reference documentation instead:

- [Reading Data](/database/reading-data/reading-data.mdx)
- [Indexes](/database/reading-data/indexes/indexes.md)

## A Library of Documents

You can imagine that Convex is a physical library storing documents as physical
books. In this world, every time you add a document to Convex with
[`db.insert("books", {...})`](/api/interfaces/server.GenericDatabaseWriter#insert)
a librarian places the book on a shelf.

By default, Convex organizes your documents in the order they were inserted. You
can imagine the librarian inserting documents left to right on a shelf.

If you run a query to find the first book like:

```ts
const firstBook = await ctx.db.query("books").first();
```

then the librarian could start at the left edge of the shelf and find the first
book. This is an extremely fast query because the librarian only has to look at
a single book to get the result.

Similarly, if we want to retrieve the last book that was inserted we could
instead do:

```ts
const lastBook = await ctx.db.query("books").order("desc").first();
```

This is the same query but we've swapped the order to descending. In the
library, this means that the librarian will start on the right edge of the shelf
and scan right-to-left. The librarian still only needs to look at a single book
to determine the result so this query is also extremely fast.

## Full Table Scans

Now imagine that someone shows up at the library and asks "what books do you
have by Jane Austen?"

This could be expressed as:

```ts
const books = await ctx.db
  .query("books")
  .filter((q) => q.eq(q.field("author"), "Jane Austen"))
  .collect();
```

This query is saying "look through all of the books, left-to-right, and collect
the ones where the `author` field is Jane Austen." To do this the librarian will
need to look through the entire shelf and check the author of every book.

This query is a _full table scan_ because it requires Convex to look at every
document in the table. The performance of this query is based on the number of
books in the library.

If your Convex table has a small number of documents, this is fine! Full table
scans should still be fast if there are a few hundred documents, but if the
table has many thousands of documents these queries will become slow.

In the library analogy, this kind of query is fine if the library has a single
shelf. As the library expands into a bookcase with many shelves or many
bookcases, this approach becomes infeasible.

## Card Catalogs

How can we more efficiently find books given an author?

One option is to re-sort the entire library by `author`. This will solve our
immediate problem but now our original queries for `firstBook` and `lastBook`
would become full table scans because we'd need to examine every book to see
which was inserted first/last.

Another option is to duplicate the entire library. We could purchase 2 copies of
every book and put them on 2 separate shelves: one shelf sorted by insertion
time and another sorted by author. This would work, but it's expensive. We now
need twice as much space for our library.

A better option is to build an _index_ on `author`. In the library, we could use
an old-school [card catalog](https://en.wikipedia.org/wiki/Library_catalog) to
organize the books by author. The idea here is that the librarian will write an
index card for each book that contains:

- The book's author
- The location of the book on the shelves

These index cards will be sorted by author and live in a separate organizer from
the shelves that hold the books. The card catalog should stay small because it
only has an index card per book (not the entire text of the book).

![Card Catalog](/img/card-catalog.jpg)

When a patron asks for "books by Jane Austen", the librarian can now:

1. Go to the card catalog and quickly find all of the cards for "Jane Austen".
2. For each card, go and find the book on the shelf.

This is quite fast because the librarian can quickly find the index cards for
Jane Austen. It's still a little bit of work to find the book for each card but
the number of index cards is small so this is quite fast.

## Indexes

Database indexes work based on the same concept! With Convex you can define an
_index_ with:

```ts noDialect title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  books: defineTable({
    author: v.string(),
    title: v.string(),
    text: v.string(),
  }).index("by_author", ["author"]),
});
```

then Convex will create a new index called `by_author` on `author`. This means
that your `books` table will now have an additional data structure that is
sorted by the `author` field.

You can query this index with:

```ts
const austenBooks = await ctx.db
  .query("books")
  .withIndex("by_author", (q) => q.eq("author", "Jane Austen"))
  .collect();
```

This query instructs Convex to go to the `by_author` index and find all the
entries where `doc.author === "Jane Austen"`. Because the index is sorted by
`author`, this is a very efficient operation. This means that Convex can execute
this query in the same manner that the librarian can:

1. Find the range of the index with entries for Jane Austen.
2. For each entry in that range, get the corresponding document.

The performance of this query is based on the number of documents where
`doc.author === "Jane Austen"` which should be quite small. We've dramatically
sped up the query!

## Backfilling and Maintaining Indexes

One interesting detail to think about is the work needed to create this new
structure. In the library, the librarian must go through every book on the shelf
and put a new index card for each one in the card catalog sorted by author. Only
after that can the librarian trust that the card catalog will give it correct
results.

The same is true for Convex indexes! When you define a new index, the first time
you run `npx convex deploy` Convex will need to loop through all of your
documents and index each one. This is why the first deploy after the creation of
a new index will be slightly slower than normal; Convex has to do a bit of work
for each document in your table.

Similarly, even after an index is defined, Convex will have to do a bit of extra
work to keep this index up to date as the data changes. Every time a document is
inserted, updated, or deleted in an indexed table, Convex will also update its
index entry. This is analogous to a librarian creating new index cards for new
books as they add them to the library.

If you are defining a few indexes there is no need to worry about the
maintenance cost. As you define more indexes, the cost to maintain them grows
because every `insert` needs to update every index. This is why Convex has a
limit of 32 indexes per table. In practice most applications define a handful of
indexes per table to make their important queries efficient.

## Indexing Multiple Fields

Now imagine that a patron shows up at the library and would like to check out
_Foundation_ by Isaac Asimov. Given our index on `author`, we can write a query
that uses the index to find all the books by Isaac Asimov and then examines the
title of each book to see if it's _Foundation_.

```ts
const foundation = await ctx.db
  .query("books")
  .withIndex("by_author", (q) => q.eq("author", "Isaac Asimov"))
  .filter((q) => q.eq(q.field("title"), "Foundation"))
  .unique();
```

This query describes how a librarian might execute the query. The librarian will
use the card catalog to find all of the index cards for Isaac Asimov's books.
The cards themselves don't have the title of the book so the librarian will need
to find every Asimov book on the shelves and look at its title to find the one
named _Foundation_. Lastly, this query ends with
[`.unique`](/api/interfaces/server.Query#unique) because we expect there to be
at most one result.

This query demonstrates the difference between filtering using
[`withIndex`](/api/interfaces/server.QueryInitializer#withindex) and
[`filter`](/api/interfaces/server.Query#filter). `withIndex` only allows you to
restrict your query based on the index. You can only do operations that the
index can do efficiently like finding all documents with a given author.

`filter` on the other hand allows you to write arbitrary, complex expressions
but it won't be run using the index. Instead, `filter` expressions will be
evaluated on every document in the range.

Given all of this, we can conclude that **the performance of indexed queries is
based on how many documents are in the index range**. In this case, the
performance is based on the number of Isaac Asimov books because the librarian
will need to look at each one to examine its title.

Unfortunately, Isaac Asimov wrote
[a lot of books](<https://en.wikipedia.org/wiki/Isaac_Asimov_bibliography_(alphabetical)>).
Realistically even with 500+ books, this will be fast enough on Convex with the
existing index, but let's consider how we could improve it anyway.

One approach is to build a separate `by_title` index on `title`. This could let
us swap the work we do in `.filter` and `.withIndex` to instead be:

```ts
const foundation = await ctx.db
  .query("books")
  .withIndex("by_title", (q) => q.eq("title", "Foundation"))
  .filter((q) => q.eq(q.field("author"), "Isaac Asimov"))
  .unique();
```

In this query, we're efficiently using the index to find all the books called
_Foundation_ and then filtering through to find the one by Isaac Asimov.

This is okay, but we're still at risk of having a slow query because too many
books have a title of _Foundation_. An even better approach could be to build a
_compound_ index that indexes both `author` and `title`. Compound indexes are
indexes on an ordered list of fields.

```ts noDialect title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  books: defineTable({
    author: v.string(),
    title: v.string(),
    text: v.string(),
  }).index("by_author_title", ["author", "title"]),
});
```

In this index, books are sorted first by the author and then within each author
by title. This means that a librarian can use the index to jump to the Isaac
Asimov section and quickly find _Foundation_ within it.

Expressing this as a Convex query this looks like:

```ts
const foundation = await ctx.db
  .query("books")
  .withIndex("by_author_title", (q) =>
    q.eq("author", "Isaac Asimov").eq("title", "Foundation"),
  )
  .unique();
```

Here the index range expression tells Convex to only consider documents where
the author is Isaac Asimov and the title is _Foundation_. This is only a single
document so this query will be quite fast!

Because this index sorts by `author` and then by `title`, it also efficiently
supports queries like "All books by Isaac Asimov that start with F." We could
express this as:

```ts
const asimovBooksStartingWithF = await ctx.db
  .query("books")
  .withIndex("by_author_title", (q) =>
    q.eq("author", "Isaac Asimov").gte("title", "F").lt("title", "G"),
  )
  .collect();
```

This query uses the index to find books where
`author === "Isaac Asimov" && "F" <= title < "G"`. Once again, the performance
of this query is based on how many documents are in the index range. In this
case, that's just the Asimov books that begin with "F" which is quite small.

Also note that this index also supports our original query for "books by Jane
Austen." It's okay to only use the `author` field in an index range expression
and not restrict by title at all.

Lastly, imagine that a library patron asks for the book _The Three-Body Problem_
but they don't know the author's name. Our `by_author_title` index won't help us
here because it's sorted first by `author`, and then by `title`. The title, _The
Three-Body Problem_, could appear anywhere in the index!

The Convex TypeScript types in the `withIndex` make this clear because they
require that you compare index fields in order. Because the index is defined on
`["author", "title"]`, you must first compare the `author` with `.eq` before the
`title`.

In this case, the best option is probably to create the separate `by_title`
index to facilitate this query.

## Conclusions

Congrats! You now understand how queries and indexes work within Convex!

Here are the main points we've covered:

1. By default Convex queries are _full table scans_. This is appropriate for
   prototyping and querying small tables.
2. As your tables grow larger, you can improve your query performance by adding
   _indexes_. Indexes are separate data structures that order your documents for
   fast querying.
3. In Convex, queries use the _`withIndex`_ method to express the portion of the
   query that uses the index. The performance of a query is based on how many
   documents are in the index range expression.
4. Convex also supports _compound indexes_ that index multiple fields.

To learn more about queries and indexes, check out our reference documentation:

- [Reading Data](/database/reading-data/reading-data.mdx)
- [Indexes](/database/reading-data/indexes/indexes.md)



================================================
FILE: npm-packages/docs/docs/database/reading-data/indexes/indexes.md
================================================
---
title: "Indexes"
sidebar_position: 100
description: "Speed up queries with database indexes"
---

Indexes are a data structure that allow you to speed up your
[document queries](/database/reading-data/reading-data.mdx#querying-documents)
by telling Convex how to organize your documents. Indexes also allow you to
change the order of documents in query results.

For a more in-depth introduction to indexing see
[Indexes and Query Performance](/database/reading-data/indexes/indexes-and-query-perf.md).

## Defining indexes

Indexes are defined as part of your Convex [schema](/database/schemas.mdx). Each
index consists of:

1. A name.
   - Must be unique per table.
2. An ordered list of fields to index.
   - To specify a field on a nested document, use a dot-separated path like
     `properties.name`.

To add an index onto a table, use the
[`index`](/api/classes/server.TableDefinition#index) method on your table's
schema:

```ts noDialect title="convex/schema.ts"
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

// Define a messages table with two indexes.
export default defineSchema({
  messages: defineTable({
    channel: v.id("channels"),
    body: v.string(),
    user: v.id("users"),
  })
    .index("by_channel", ["channel"])
    .index("by_channel_user", ["channel", "user"]),
});
```

The `by_channel` index is ordered by the `channel` field defined in the schema.
For messages in the same channel, they are ordered by the
[system-generated `_creationTime` field](/database/types.md#system-fields) which
is added to all indexes automatically.

By contrast, the `by_channel_user` index orders messages in the same `channel`
by the `user` who sent them, and only then by `_creationTime`.

Indexes are created in [`npx convex dev`](/cli.md#run-the-convex-dev-server) and
[`npx convex deploy`](/cli.md#deploy-convex-functions-to-production).

You may notice that the first deploy that defines an index is a bit slower than
normal. This is because Convex needs to _backfill_ your index. The more data in
your table, the longer it will take Convex to organize it in index order. If
this is problematic for your workflow, [contact us](/production/contact.md).

You can feel free to query an index in the same deploy that defines it. Convex
will ensure that the index is backfilled before the new query and mutation
functions are registered.

<Admonition type="caution" title="Be careful when removing indexes">

In addition to adding new indexes, `npx convex deploy` will delete indexes that
are no longer present in your schema. Make sure that your indexes are completely
unused before removing them from your schema!

</Admonition>

## Querying documents using indexes

A query for "messages in `channel` created 1-2 minutes ago" over the
`by_channel` index would look like:

```ts
const messages = await ctx.db
  .query("messages")
  .withIndex("by_channel", (q) =>
    q
      .eq("channel", channel)
      .gt("_creationTime", Date.now() - 2 * 60000)
      .lt("_creationTime", Date.now() - 60000),
  )
  .collect();
```

The [`.withIndex`](/api/interfaces/server.QueryInitializer#withindex) method
defines which index to query and how Convex will use that index to select
documents. The first argument is the name of the index and the second is an
_index range expression_. An index range expression is a description of which
documents Convex should consider when running the query.

The choice of index both affects how you write the index range expression and
what order the results are returned in. For instance, by making both a
`by_channel` and `by_channel_user` index, we can get results within a channel
ordered by `_creationTime` or by `user`, respectively. If you were to use the
`by_channel_user` index like this:

```ts
const messages = await ctx.db
  .query("messages")
  .withIndex("by_channel_user", (q) => q.eq("channel", channel))
  .collect();
```

The results would be all of the messages in a `channel` ordered by `user`, then
by `_creationTime`. If you were to use `by_channel_user` like this:

```ts
const messages = await ctx.db
  .query("messages")
  .withIndex("by_channel_user", (q) =>
    q.eq("channel", channel).eq("user", user),
  )
  .collect();
```

The results would be the messages in the given `channel` sent by `user`, ordered
by `_creationTime`.

An index range expression is always a chained list of:

1. 0 or more equality expressions defined with
   [`.eq`](/api/interfaces/server.IndexRangeBuilder#eq).
2. [Optionally] A lower bound expression defined with
   [`.gt`](/api/interfaces/server.IndexRangeBuilder#gt) or
   [`.gte`](/api/interfaces/server.IndexRangeBuilder#gte).
3. [Optionally] An upper bound expression defined with
   [`.lt`](/api/interfaces/server.IndexRangeBuilder#lt) or
   [`.lte`](/api/interfaces/server.IndexRangeBuilder#lte).

**You must step through fields in index order.**

Each equality expression must compare a different index field, starting from the
beginning and in order. The upper and lower bounds must follow the equality
expressions and compare the next field.

For example, it is not possible to write a query like:

```ts
// DOES NOT COMPILE!
const messages = await ctx.db
  .query("messages")
  .withIndex("by_channel", (q) =>
    q
      .gt("_creationTime", Date.now() - 2 * 60000)
      .lt("_creationTime", Date.now() - 60000),
  )
  .collect();
```

This query is invalid because the `by_channel` index is ordered by
`(channel, _creationTime)` and this query range has a comparison on
`_creationTime` without first restricting the range to a single `channel`.
Because the index is sorted first by `channel` and then by `_creationTime`, it
isn't a useful index for finding messages in all channels created 1-2 minutes
ago. The TypeScript types within `withIndex` will guide you through this.

To better understand what queries can be run over which indexes, see
[Introduction to Indexes and Query Performance](/database/reading-data/indexes/indexes-and-query-perf.md).

**The performance of your query is based on the specificity of the range.**

For example, if the query is

```ts
const messages = await ctx.db
  .query("messages")
  .withIndex("by_channel", (q) =>
    q
      .eq("channel", channel)
      .gt("_creationTime", Date.now() - 2 * 60000)
      .lt("_creationTime", Date.now() - 60000),
  )
  .collect();
```

then query's performance would be based on the number of messages in `channel`
created 1-2 minutes ago.

If the index range is not specified, all documents in the index will be
considered in the query.

<Admonition type="tip" title="Picking a good index range">

For performance, define index ranges that are as specific as possible! If you
are querying a large table and you're unable to add any equality conditions with
`.eq`, you should consider defining a new index.

</Admonition>

`.withIndex` is designed to only allow you to specify ranges that Convex can
efficiently use your index to find. For all other filtering you can use the
[`.filter`](/api/interfaces/server.Query#filter) method.

For example to query for "messages in `channel` **not** created by me" you could
do:

```ts
const messages = await ctx.db
  .query("messages")
  .withIndex("by_channel", q => q.eq("channel", channel))
  .filter(q => q.neq(q.field("user"), myUserId)
  .collect();
```

In this case the performance of this query will be based on how many messages
are in the channel. Convex will consider each message in the channel and only
return the messages where the `user` field matches `myUserId`.

## Sorting with indexes

Queries that use `withIndex` are ordered by the columns specified in the index.

The order of the columns in the index dictates the priority for sorting. The
values of the columns listed first in the index are compared first. Subsequent
columns are only compared as tie breakers only if all earlier columns match.

Since Convex automatically includes `_creationTime` as the last column in all
indexes, `_creationTime` will always be the final tie breaker if all other
columns in the index are equal.

For example, `by_channel_user` includes `channel`, `user`, and `\_creationTime`.
So queries on `messages` that use `.withIndex("by_channel_user")` will be sorted
first by channel, then by user within each channel, and finally by the creation
time.

Sorting with indexes allows you to satisfy use cases like displaying the top `N`
scoring users, the most recent `N` transactions, or the most `N` liked messages.

For example, to get the top 10 highest scoring players in your game, you might
define an index on the player's highest score:

```ts
export default defineSchema({
  players: defineTable({
    username: v.string(),
    highestScore: v.number(),
  }).index("by_highest_score", ["highestScore"]),
});
```

You can then efficiently find the top 10 highest scoring players using your
index and [`take(10)`](/api/interfaces/server.Query#take):

```ts
const topScoringPlayers = await ctx.db
  .query("users")
  .withIndex("by_highest_score")
  .order("desc")
  .take(10);
```

In this example, the range expression is omitted because we're looking for the
highest scoring players of all time. This particular query is reasonably
efficient for large data sets only because we're using `take()`.

If you use an index without a range expression, you should always use one of the
following in conjunction with `withIndex`:

1. [`.first()`](/api/interfaces/server.Query#first)
2. [`.unique()`](/api/interfaces/server.Query#unique)
3. [`.take(n)`](/api/interfaces/server.Query#take)
4. [`.paginate(ops)`](/database/pagination.mdx)

These APIs allow you to efficiently limit your query to a reasonable size
without performing a full table scan.

<Admonition type="caution" title="Full Table Scans">

When your query fetches documents from the database, it will scan the rows in
the range you specify. If you are using `.collect()`, for instance, it will scan
all of the rows in the range. So if you use `withIndex` without a range
expression, you will be
[scanning the whole table](https://docs.convex.dev/database/indexes/indexes-and-query-perf#full-table-scans),
which can be slow when your table has thousands of rows. `.filter()` doesn't
affect which documents are scanned. Using `.first()` or `.unique()` or
`.take(n)` will only scan rows until it has enough documents.

</Admonition>

You can include a range expression to satisfy more targeted queries. For
example, to get the top scoring players in Canada, you might use both `take()`
and a range expression:

```ts
// query the top 10 highest scoring players in Canada.
const topScoringPlayers = await ctx.db
  .query("users")
  .withIndex("by_country_highest_score", (q) => q.eq("country", "CA"))
  .order("desc")
  .take(10);
```

## Limits

Convex supports indexes containing up to 16 fields. You can define 32 indexes on
each table. Indexes can't contain duplicate fields.

No reserved fields (starting with `_`) are allowed in indexes. The
`_creationTime` field is automatically added to the end of every index to ensure
a stable ordering. It should not be added explicitly in the index definition,
and it's counted towards the index fields limit.

The `by_creation_time` index is created automatically (and is what is used in
database queries that don't specify an index). The `by_id` index is reserved.


